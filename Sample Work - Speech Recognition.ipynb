{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SPEECH RECOGNITION USING DYNAMIC TIME WARPING\n",
    "\n",
    "CHINMAY BAKE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import euclidean\n",
    "from fastdtw import fastdtw\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "from python_speech_features import mfcc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### INTRODUCTION\n",
    "\n",
    "Voice commands can be given to drive or actuate a system. Speech recognition is the primary requirement for the said task. Speech recognition is ability of a machine or A program to identify words and phrases in spoken language and convert them into a machine-readable format.The recognition process involves processing of the input command and the library commands and then matching them by using a suitable matching algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate(amplitude_1,frequency_1,amplitude_2,frequency_2,samples):\n",
    "    \n",
    "    x = np.arange(samples)\n",
    "    \n",
    "    signal_1 = amplitude_1 * np.sin(2 * np.pi * frequency_1 * x/44100)\n",
    "    signal_2 = amplitude_2 * np.sin(2 * np.pi * frequency_2 * x/44100)\n",
    "    \n",
    "    return (signal_1, signal_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For prototyping this proof-of-concept, we use simulated signals instead of real ones. The simulation includes signals with varying amplitudes and frequencies. The first step in feature engineering is to compute MFCC of the data. MFCC takes into account human perception for sensitivity at appropriate frequencies by converting the conventional frequency to Mel Scale, and are thus suitable for speech recognition tasks quite well. Below are the steps on how to compute MFCC - \n",
    "\n",
    "![alt text](sr1.png \"Title\")\n",
    "\n",
    "Source:Semanticscholar.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity(series1,series2): \n",
    "    \n",
    "    mfcc1 = mfcc(series1)\n",
    "    mfcc2 = mfcc(series2)\n",
    "    \n",
    "    distance, path = fastdtw(mfcc1, mfcc2, dist=euclidean)\n",
    "    \n",
    "    return distance\n",
    "  \n",
    "def similar_series(z): \n",
    "    \n",
    "    closest = []\n",
    "\n",
    "    for i in range(10,101,10):\n",
    "        value = z[i-10:i].sort_values(by = 0, ascending = True)[1:2].index[0]\n",
    "        closest.append(value)\n",
    "\n",
    "    for k in range(0,10,1):\n",
    "\n",
    "        if closest[k] < 10:\n",
    "            print(closest[k])\n",
    "        else:\n",
    "            print(str(closest[k])[0] + '-->' + str(closest[k])[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DYNAMIC TIME WARPING\n",
    "\n",
    "We utilise this method for matching the feature engineered mfcc data points. it is widely used in comparing and evaluating temporal data. It works by first evaluating the least cost distance to reach every square in the grid and then tracing back the path which corresponds to the overall least cost distance.\n",
    "\n",
    "Below is the algorithm in a nutshell: \n",
    "\n",
    "![alt text](sr2.png \"Title\")\n",
    "\n",
    "#### PROOF OF CONCEPT - DYNAMIC TIME WARPING IN CONJUCTION WITH MFCC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](sr5.png \"Title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1-->3\n",
      "2-->6\n",
      "3-->1\n",
      "4-->3\n",
      "5-->6\n",
      "6-->5\n",
      "7-->9\n",
      "8-->6\n",
      "9-->7\n"
     ]
    }
   ],
   "source": [
    "def check_series():\n",
    "    \n",
    "    no_of_series = 10\n",
    "    freq = []\n",
    "    amplitude = []\n",
    "    sim = []\n",
    "\n",
    "    for i in range(0,no_of_series,1):\n",
    "        freq.append(np.random.randint(5,10))\n",
    "        amplitude.append(np.random.randint(20,80))\n",
    "    \n",
    "    for i in range(0,no_of_series,1):\n",
    "        for j in range(0,no_of_series,1):\n",
    "            x = simulate(freq[i],amplitude[i],freq[j],amplitude[j],100)\n",
    "            sim.append(similarity(x[0],x[1]))\n",
    "            \n",
    "    return similar_series(pd.DataFrame(sim))\n",
    "\n",
    "check_series()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### REFERENCES\n",
    "\n",
    "- Baum, L. E., Petrie, T., Soules, G. & Weiss, N. (1970), ‘A maximization technique occurring in the statistical analysis of probabilistic functions of Markov chains’, Ann.Math. Sastist.\n",
    "- Blum, T. L., Keislar, D. F., Wheaton, J. A. & Wold, E. H. (1999), Method and article of manufacture for content-based analysis, storage, retrieval, and segmentation of audio information, U.S Patent 5, 918, 223\n",
    "- Foote, J. T. (1997), Content-based retrieval of music and audio, in ‘SPIE’, pp. 138 – 147\n",
    "- Logan, B. T. & Chu, S. (2000), Music summarization using key phrases, in ‘Proceedings IEEE Internal Conference on Acoustics, Speech, and Signal Processing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
